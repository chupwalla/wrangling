{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit ('3.9.0': pyenv)"
  },
  "interpreter": {
   "hash": "c06359dc1d5621306d96d87f584b93704713634a37dc11220189996a7c90f90b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Gather\n",
    "\n",
    "- Twitter Archive Enhanced downloaded from Resources, saved as twitter-archive-enhanced.csv\n",
    "\n",
    "- Image Predictions downloaded programmatically from Udacity using the Requests library, saved as image-predictions.tsv\n",
    "\n",
    "- JSON data for every tweet in the archive downloaded programmatically using the Tweepy library and Twitter API, saved as tweet_json.txt"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Assess\n",
    "\n",
    "- Visually\n",
    "\n",
    "- Programmatically\n",
    "\n",
    "- Detect and document EIGHT Quality issues and TWO tidiness issues (must satisfy the Project Motivation Key Points)\n",
    "\n",
    "## Key Points:\n",
    "\n",
    "- You only want original ratings (no retweets) that have images. Though there are 5000+ tweets in the dataset, not all are dog ratings and some are retweets.\n",
    "- Assessing and cleaning the entire dataset completely would require a lot of time, and is not necessary to practice and demonstrate your skills in data wrangling. Therefore, the requirements of this project are only to assess and clean at least 8 quality issues and at least 2 tidiness issues in this dataset.\n",
    "- Cleaning includes merging individual pieces of data according to the rules of tidy data.\n",
    "- The fact that the rating numerators are greater than the denominators does not need to be cleaned. This unique rating system is a big part of the popularity of WeRateDogs.\n",
    "- You do not need to gather the tweets beyond August 1st, 2017. You can, but note that you won't be able to gather the image predictions for these tweets since you don't have access to the algorithm used."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Clean\n",
    "\n",
    "- Clean data and place in Pandas dataframe"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Storing, Analyzing, and Visualizing Data\n",
    "\n",
    "- Store the clean dataframes in CSV file(s), the main file named twitter_archive_master.csv\n",
    "\n",
    "- Optionally, store the data in a SQLite database (submit if it exists)\n",
    "\n",
    "- Analyze and Visualize data in the wrangle_act notebook"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Import required libraries'''\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import seaborn as sb\n",
    "import tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Use the Requests library to download the image-predictions TSV file from Udacity'''\n",
    "\n",
    "r = requests.get('https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv')\n",
    "print(r.status_code)\n",
    "\n",
    "with open('image-predictions.tsv', 'w') as outfile:\n",
    "    outfile.write(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Use the Tweepy library and Twitter API to download extended data for the tweets in the archive'''\n",
    "\n",
    "CONSUMER_KEY = os.environ.get('CONSUMER_KEY')\n",
    "CONSUMER_SECRET = os.environ.get('CONSUMER_SECRET')\n",
    "ACCESS_TOKEN = os.environ.get('ACCESS_TOKEN')\n",
    "ACCESS_SECRET = os.environ.get('ACCESS_SECRET')\n",
    "\n",
    "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET)\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "\n",
    "tweet_df = pd.read_csv('twitter-archive-enhanced.csv')\n",
    "tweets_ids = list(tweet_df.tweet_id)\n",
    "\n",
    "def get_tweets(tweet_ids):\n",
    "    '''Function to take tweet IDs in list form and create a dictionary to pass as JSON to a .txt file'''\n",
    "\n",
    "    nonexistent = list()\n",
    "    tweets = dict()\n",
    "    for tweet_id in tweet_ids:\n",
    "        print(tweet_id)\n",
    "        try:\n",
    "            twt = api.get_status(tweet_id, tweet_mode='extended')\n",
    "            tweets[tweet_id] = twt._json\n",
    "        except tweepy.TweepError:\n",
    "            nonexistent.append(tweet_id)\n",
    "    print(nonexistent)\n",
    "    return tweets\n",
    "\n",
    "t = get_tweets(tweets_ids)\n",
    "\n",
    "with open('tweet_json.txt', 'w') as outfile:\n",
    "    json.dump(t, outfile)"
   ]
  },
  {
   "source": [
    "# ASSESSMENT\n",
    "\n",
    "- Some tweets in the archive are actually retweets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}